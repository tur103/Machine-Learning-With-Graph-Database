{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine-Learning-With-Graph-Database.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9LU3b/mXvl5GSWTFTp28t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tur103/Social-Network-Link-Prediction/blob/master/Machine_Learning_With_Graph_Database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqZJdxh9j4L7",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning With Graph Database\n",
        "\n",
        "---\n",
        "\n",
        "### Welcome to the machine learning with graph database tutorial\n",
        "\n",
        "In this notebook you will learn how to build a machine learning based on graph database.\n",
        "\n",
        "In the project we will use Neo4j graph database platform, pandas data science library and scikit-learn models building library to create a machine learning classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-hLFXfjqkIX",
        "colab_type": "text"
      },
      "source": [
        "## Predictions\n",
        "\n",
        "Our machine learning model should be able to predict what would be the right answer of a given question according to what it has learnt so far.\n",
        "\n",
        "In graph database it is the ability to predict a near future connection between two entities that are currently not connected.\n",
        "\n",
        "### This is called Link-Prediction\n",
        "\n",
        "In other words:\n",
        "Given a snapshot of a network, can we infer which new interactions among its members are likely to occur in the near future?\n",
        "\n",
        "In this guide we will learn how to build a machine learning classifier to predict future relationships between people in a social network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-ZdoOivFzAN",
        "colab_type": "text"
      },
      "source": [
        "## Database\n",
        "\n",
        "You will need to create a clouded Neo4j database instance using Neo4j Sandbox. Its free! Here is a link for instructions: [tap on me](https://youtu.be/rmfgRKPjhl8)\n",
        "\n",
        "After lunching your sandbox, create a blank database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwrWawYvvTBx",
        "colab_type": "text"
      },
      "source": [
        "## Install Libraries\n",
        "\n",
        "First, we need to install the following libraries for our project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JjDF95pvfDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install py2neo==4.1.3 pandas matplotlib sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsWsBu802Wap",
        "colab_type": "text"
      },
      "source": [
        "## Start to Code\n",
        "\n",
        "Next, we want to import our libraries into the interpreter and validate their installation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE3dD2Ai2fYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from py2neo import Graph\n",
        "import pandas as pd\n",
        "import statistics\n",
        "\n",
        "import matplotlib \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA5Rhb3G2mPb",
        "colab_type": "text"
      },
      "source": [
        "Now we need to create a connection to our graph instance using py2neo.\n",
        "You will need to update the connection details to the graph according to the ones appeared in the created database on your sandbox."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB7r3IBE5t5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change the line of code below to use the Bolt URL and Password of your Neo4j instace.\n",
        "# graph = Graph(\"bolt://<IP Address>:<Bolt Port>\", auth=(\"neo4j\", \"<Password>\")) \n",
        " \n",
        "graph = Graph(\"bolt://54.162.238.103:32784\", auth=(\"neo4j\", \"market-waste-linkages\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4bvQ2GDv_Jj",
        "colab_type": "text"
      },
      "source": [
        "## Build Social Network Graph\n",
        "\n",
        "In order to build the social network graph database, here is a pre-prepared script that runs some cypher queries to load all the data we need into the graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyVp2gsXHNxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_education = \"\"\"CREATE INDEX ON :Education(name)\"\"\"\n",
        "index_hiring_source = \"\"\"CREATE INDEX ON :HiringSource(name)\"\"\"\n",
        "index_job = \"\"\"CREATE INDEX ON :Job(name)\"\"\"\n",
        "index_location = \"\"\"CREATE INDEX ON :Location(name)\"\"\"\n",
        "index_person = \"\"\"CREATE INDEX ON :Person(name)\"\"\"\n",
        "index_team = \"\"\"CREATE INDEX ON :Team(name)\"\"\"\n",
        "index_university = \"\"\"CREATE INDEX ON :University(name)\"\"\"\n",
        "\n",
        "load_person = \"\"\"\n",
        "LOAD CSV WITH HEADERS FROM \"https://github.com/tur103/Social-Network-Link-Prediction/raw/master/data/People.csv\" AS line\n",
        "WITH line LIMIT 800\n",
        "\n",
        "MERGE (person:Person {name: line.Name})\n",
        "SET person += line\n",
        "WITH line, person\n",
        "\n",
        "CALL apoc.do.when(line.Retirement = \"Yes\",\n",
        "\t\t\t\t  'SET person:Retired RETURN person',\n",
        "                  'RETURN person',\n",
        "                  {line:line, person:person})\n",
        "YIELD value\n",
        "WITH line, value[\"person\"] AS person\n",
        "\n",
        "CALL apoc.do.when(line.`Is.Manager` = \"Yes\",\n",
        "\t\t\t\t  'SET person:Manager RETURN person',\n",
        "                  'RETURN person',\n",
        "                  {line:line, person:person})\n",
        "YIELD value\n",
        "WITH line, value[\"person\"] AS person\n",
        "\n",
        "CALL apoc.create.addLabels(person, [line.Sex]) YIELD node\n",
        "WITH COLLECT(node) AS nodes\n",
        "RETURN 1\n",
        "\"\"\"\n",
        "load_education = \"\"\"\n",
        "MATCH (person:Person)\n",
        "MERGE (education:Education {name: person.`Education.Level`})\n",
        "MERGE (person)-[:EDUCATION_LEVEL]->(education)\n",
        "\"\"\"\n",
        "load_hiring_source = \"\"\"\n",
        "MATCH (person:Person)\n",
        "MERGE (hiring_source:HiringSource {name: person.`Hiring.Source`})\n",
        "MERGE (person)-[:HIRING_SOURCE]->(hiring_source)\n",
        "\"\"\"\n",
        "load_job = \"\"\"\n",
        "MATCH (person:Person)\n",
        "MERGE (job:Job {name: person.`Job.Family`})\n",
        "MERGE (person)-[:JOB]->(job)\n",
        "\"\"\"\n",
        "load_location = \"\"\"\n",
        "MATCH (person:Person)\n",
        "MERGE (location:Location {name: person.`Biz.Location`})\n",
        "MERGE (person)-[:LOACTION]->(location)\n",
        "\"\"\"\n",
        "load_team = \"\"\"\n",
        "MATCH (person:Person)\n",
        "MERGE (team:Team {name: person.Team})\n",
        "MERGE (person)-[:TEAM]->(team)\n",
        "\"\"\"\n",
        "load_university = \"\"\"\n",
        "MATCH (person:Person)\n",
        "MERGE (university:University {name: person.University})\n",
        "MERGE (person)-[:STUDIED_AT]->(university)\n",
        "\"\"\"\n",
        "\n",
        "connect_education = \"\"\"\n",
        "MATCH (education:Education)\n",
        "MATCH (person:Person)-[:EDUCATION_LEVEL]->(education)<-[:EDUCATION_LEVEL]-(other_person:Person)\n",
        "WHERE ID(person) > ID(other_person)\n",
        "MERGE (person)<-[r:RELATED {employ_id: (toInteger(person.`Emp.No`) + toInteger(other_person.`Emp.No`))}]->(other_person)\n",
        "ON CREATE SET r.value = 2\n",
        "ON MATCH SET r.value = r.value + 2\n",
        "\"\"\"\n",
        "connect_hiring_source = \"\"\"\n",
        "MATCH (hiring_source:HiringSource)\n",
        "MATCH (person:Person)-[:HIRING_SOURCE]->(hiring_source)<-[:HIRING_SOURCE]-(other_person:Person)\n",
        "WHERE ID(person) > ID(other_person)\n",
        "MERGE (person)<-[r:RELATED {employ_id: (toInteger(person.`Emp.No`) + toInteger(other_person.`Emp.No`))}]->(other_person)\n",
        "ON CREATE SET r.value = 1\n",
        "ON MATCH SET r.value = r.value + 1\n",
        "\"\"\"\n",
        "connect_job = \"\"\"\n",
        "MATCH (job:Job)\n",
        "MATCH (person:Person)-[:JOB]->(job)<-[:JOB]-(other_person:Person)\n",
        "WHERE ID(person) > ID(other_person)\n",
        "MERGE (person)<-[r:RELATED {employ_id: (toInteger(person.`Emp.No`) + toInteger(other_person.`Emp.No`))}]->(other_person)\n",
        "ON CREATE SET r.value = 4\n",
        "ON MATCH SET r.value = r.value + 4\n",
        "\"\"\"\n",
        "connect_location = \"\"\"\n",
        "MATCH (location:Location)\n",
        "MATCH (person:Person)-[:LOACTION]->(location)<-[:LOACTION]-(other_person:Person)\n",
        "WHERE ID(person) > ID(other_person)\n",
        "MERGE (person)<-[r:RELATED {employ_id: (toInteger(person.`Emp.No`) + toInteger(other_person.`Emp.No`))}]->(other_person)\n",
        "ON CREATE SET r.value = 5\n",
        "ON MATCH SET r.value = r.value + 5\n",
        "\"\"\"\n",
        "connect_team = \"\"\"\n",
        "MATCH (team:Team)\n",
        "MATCH (person:Person)-[:TEAM]->(team)<-[:TEAM]-(other_person:Person)\n",
        "WHERE ID(person) > ID(other_person)\n",
        "MERGE (person)<-[r:RELATED {employ_id: (toInteger(person.`Emp.No`) + toInteger(other_person.`Emp.No`))}]->(other_person)\n",
        "ON CREATE SET r.value = 6\n",
        "ON MATCH SET r.value = r.value + 6\n",
        "\"\"\"\n",
        "connect_university = \"\"\"\n",
        "MATCH (university:University)\n",
        "MATCH (person:Person)-[:STUDIED_AT]->(university)<-[:STUDIED_AT]-(other_person:Person)\n",
        "WHERE ID(person) > ID(other_person)\n",
        "MERGE (person)<-[r:RELATED {employ_id: (toInteger(person.`Emp.No`) + toInteger(other_person.`Emp.No`))}]->(other_person)\n",
        "ON CREATE SET r.value = 3\n",
        "ON MATCH SET r.value = r.value + 3\n",
        "\"\"\"\n",
        "\n",
        "remove_week_connections = \"\"\"\n",
        "MATCH (person:Person)-[r:RELATED]->(other_person:Person)\n",
        "WHERE r.value < 15\n",
        "DELETE r\n",
        "\"\"\"\n",
        "\n",
        "graph.run(cypher=index_education)\n",
        "graph.run(cypher=index_hiring_source)\n",
        "graph.run(cypher=index_job)\n",
        "graph.run(cypher=index_location)\n",
        "graph.run(cypher=index_person)\n",
        "graph.run(cypher=index_team)\n",
        "graph.run(cypher=index_university)\n",
        "graph.run(cypher=load_person)\n",
        "graph.run(cypher=load_education)\n",
        "graph.run(cypher=load_hiring_source)\n",
        "graph.run(cypher=load_job)\n",
        "graph.run(cypher=load_location)\n",
        "graph.run(cypher=load_team)\n",
        "graph.run(cypher=load_university)\n",
        "graph.run(cypher=connect_education)\n",
        "graph.run(cypher=connect_hiring_source)\n",
        "graph.run(cypher=connect_job)\n",
        "graph.run(cypher=connect_location)\n",
        "graph.run(cypher=connect_team)\n",
        "graph.run(cypher=connect_university)\n",
        "graph.run(cypher=remove_week_connections)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qvtWjEithy_",
        "colab_type": "text"
      },
      "source": [
        "## Graph algorithms in Neo4j\n",
        "\n",
        "Neo4j has a graph algorithms library to perform advanced manipulations on the graph and gain hidden insights from the data we already have.\n",
        "Graph algorithms take into consideration the graph topology in order to extract information from the way that the entities are connected.\n",
        "\n",
        "In this tutorial we are going to use the link prediction algorithms.\n",
        "These are set of methods that compute a score for a pair of nodes, where the score could be considered a measure of proximity or “similarity” between those nodes based on the graph topology. The closer two nodes are, the more likely there will be a relationship between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72SvDwffqvCB",
        "colab_type": "text"
      },
      "source": [
        "## Link Prediction Algorithms\n",
        "\n",
        "The uniqueness of graph database compared to relational databases gives us the ability to look at our data in a form of a network.\n",
        "\n",
        "Instead of predict links between people based on their formal characteristics, we can predict relationships based on the connectedness of the people on the network.\n",
        "\n",
        "While we look at the network, it's structure and the way that our entities inside it are connected to each other, we can predict connections between people based on their connection to other people.\n",
        "\n",
        "This sort of thinkning is way more reality like. People not just become friends of other people at the same age or people that were learning the same subjects in collage. In real life people are creating new relationships based on common friends and belonging to a certain social circle.\n",
        "\n",
        "Link Prediction Algorithms do exectly this!\n",
        "They predict the availability of near future relationships based on the schema of the graph and the way that the entities are conencted to each other in the network.\n",
        "\n",
        "With link predictions on graph databases we can predict new relationships in a more life-like way to gain more accurate and intelligent results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oZdWQ2UL8WI",
        "colab_type": "text"
      },
      "source": [
        "## Supervised learning\n",
        "\n",
        "You will take the supervised learning approach where you use the scores as features to train a binary classifier. The binary classifier then predicts whether a pair of nodes will have a link.\n",
        "\n",
        "## Train and test datasets \n",
        "\n",
        "Next, you must create the train and test datasets on which you can build, and then evaluate a model.\n",
        "\n",
        "## Positive examples\n",
        "\n",
        "we need to split the graph into training and test sub graphs. We can split the graph according to the relationship's employ_id between people at the median. The training set will be from ID lower than the median, the test set from larger.\n",
        "\n",
        "Subsequently, pairs of nodes in our train and test datasets will have relationships between them. They will be the __positive examples__ in your machine learning model.\n",
        "\n",
        "Let's find out what our employ_id median is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGTXXZBdW1Ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = \"\"\"\n",
        "MATCH (person:Person)<-[r:RELATED]->(other_person:Person) \n",
        "WITH toInteger(person.`Emp.No`) + toInteger(other_person.`Emp.No`) AS employ_id\n",
        "ORDER BY employ_id\n",
        "RETURN employ_id\n",
        "\"\"\"\n",
        "employ_ids = graph.run(query).to_data_frame()[\"employ_id\"]\n",
        "\n",
        "median = statistics.median(employ_ids)\n",
        "print(median)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdnRRk9ERNsR",
        "colab_type": "text"
      },
      "source": [
        "It looks like our employ_id median is 5946. So this is where we are going to split our datasets.\n",
        "\n",
        "Let's create explicit `RELATED_BEFORE` and `RELATED_AFTER` relationships in the graph based on that employ_id. The following code will create these relationships:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n13DpczCO5vJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = \"\"\"\n",
        "MATCH (person:Person)<-[r:RELATED]->(other_person:Person) \n",
        "where toInteger(person.`Emp.No`) + toInteger(other_person.`Emp.No`) < 5946\n",
        "MERGE (person)<-[new_r:RELATED_BEFORE]->(other_person)\n",
        "SET new_r.value = r.value\n",
        "\"\"\"\n",
        "\n",
        "graph.run(query).stats()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9dAudNMR-g5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = \"\"\"\n",
        "MATCH (person:Person)<-[r:RELATED]->(other_person:Person) \n",
        "where toInteger(person.`Emp.No`) + toInteger(other_person.`Emp.No`) >= 5946\n",
        "MERGE (person)<-[new_r:RELATED_AFTER]->(other_person)\n",
        "SET new_r.value = r.value\n",
        "\"\"\"\n",
        "\n",
        "graph.run(query).stats()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tCxiQNZSKmy",
        "colab_type": "text"
      },
      "source": [
        "Determine how many RELATED relationships you have in each of these sub graphs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xabxn9eYSKDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = \"\"\"\n",
        "MATCH (:Person)<-[:RELATED_BEFORE]->(:Person)\n",
        "RETURN count(*) AS count\n",
        "\"\"\"\n",
        "\n",
        "graph.run(query).to_data_frame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRrv1HAHSTi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = \"\"\"\n",
        "MATCH (:Person)<-[:RELATED_AFTER]->(:Person)\n",
        "RETURN count(*) AS count\n",
        "\"\"\"\n",
        "\n",
        "graph.run(query).to_data_frame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL91nG0YStaG",
        "colab_type": "text"
      },
      "source": [
        "This graph has a split of nearly 50-50, which is great! Next, we create our negative examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgPKbRTbTVkU",
        "colab_type": "text"
      },
      "source": [
        "## Negative examples\n",
        "\n",
        "The simplest approach is to use all pair of nodes that don’t have a relationship. __The problem with this approach is that there are significantly more examples of pairs of nodes that don’t have a relationship than there are pairs of nodes that do__.\n",
        "\n",
        "If you were to use all of these negative examples in your training set, you would have a massive class imbalance — there are many negative examples and relatively few positive ones.\n",
        "\n",
        "You need to reduce the number of negative examples. An approach described in several link prediction papers is to use pairs of nodes that are a __specific number of hops away from each other__.\n",
        "\n",
        "This will significantly reduce the number of negative examples, although there will still be a lot more negative examples than positive.\n",
        "\n",
        "To solve this problem, you either need to down sample the negative examples or up sample the positive examples.\n",
        "\n",
        "You will take the down sampling approach. The following function will do this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTEJdzRVUC4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def down_sample(df):\n",
        "    copy = df.copy()\n",
        "    zero = Counter(copy.label.values)[0]\n",
        "    un = Counter(copy.label.values)[1]\n",
        "    n = zero - un\n",
        "    copy = copy.drop(copy[copy.label == 0].sample(n=n, random_state=1).index)\n",
        "    return copy.sample(frac=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUQsiPazURKO",
        "colab_type": "text"
      },
      "source": [
        "Now you are ready to build the train and test datasets based on the train and test sub graphs that you created. \n",
        "\n",
        "* The positive examples will be taken directly from the graph. \n",
        "* The negative examples will be found by looking for people who are 2 or 3 hops away from each other, excluding those that have already related. You will then down sample those examples to equal the size of the positive examples. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW2bwcX2UwPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_existing_links = graph.run(\"\"\"\n",
        "MATCH (person:Person)<-[:RELATED_BEFORE]->(other_person:Person)\n",
        "RETURN id(person) AS node1, id(other_person) AS node2, 1 AS label\n",
        "\"\"\").to_data_frame()\n",
        "\n",
        "train_missing_links = graph.run(\"\"\"\n",
        "MATCH (person:Person)\n",
        "WHERE (person)<-[:RELATED_BEFORE]->(:Person)\n",
        "MATCH (person)<-[:RELATED_BEFORE*2..3]->(other_person:Person)\n",
        "WHERE not((person)<-[:RELATED_BEFORE]->(other_person))\n",
        "RETURN id(person) AS node1, id(other_person) AS node2, 0 AS label\n",
        "\"\"\").to_data_frame()\n",
        "train_missing_links = train_missing_links.drop_duplicates()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Uy2VcWLVRYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_df = train_missing_links.append(train_existing_links, ignore_index=True)\n",
        "training_df['label'] = training_df['label'].astype('category')\n",
        "training_df = down_sample(training_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MME5XIi5Yvmd",
        "colab_type": "text"
      },
      "source": [
        "Now let's have a look what our train DataFrame contains:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoNwA4bZYwnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq5Te-iuY3yp",
        "colab_type": "text"
      },
      "source": [
        "Let's repeat the process for the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebkY3vu2bdYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_existing_links = graph.run(\"\"\"\n",
        "MATCH (person:Person)<-[:RELATED_AFTER]->(other_person:Person)\n",
        "RETURN id(person) AS node1, id(other_person) AS node2, 1 AS label\n",
        "\"\"\").to_data_frame()\n",
        "\n",
        "test_missing_links = graph.run(\"\"\"\n",
        "MATCH (person:Person)\n",
        "WHERE (person)<-[:RELATED_AFTER]->(:Person)\n",
        "MATCH (person)<-[:RELATED_AFTER*2..3]->(other_person:Person)\n",
        "WHERE not((person)<-[:RELATED_AFTER]->(other_person))\n",
        "RETURN id(person) AS node1, id(other_person) AS node2, 0 AS label\n",
        "\"\"\").to_data_frame()\n",
        "test_missing_links = test_missing_links.drop_duplicates()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Pyogf3cH17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = test_missing_links.append(test_existing_links, ignore_index=True)\n",
        "test_df['label'] = test_df['label'].astype('category')\n",
        "test_df = down_sample(test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33rJ1nZucNbu",
        "colab_type": "text"
      },
      "source": [
        "And it's time to sample our test DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA59FhklcPXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqG9YrmZicLZ",
        "colab_type": "text"
      },
      "source": [
        "## Choosing a machine learning algorithm\n",
        "\n",
        "Next, you will create a machine learning pipeline based on a random forest classifier. This method is well suited as this data set will be comprised of a mix of strong and weak features. While the weak features will sometimes be helpful, the random forest method will ensure that you don’t create a model that only fits the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uApDw9YFikvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = RandomForestClassifier(n_estimators=30, max_depth=10, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjiLT3IEi9m0",
        "colab_type": "text"
      },
      "source": [
        "# Generating graphy features\n",
        "\n",
        "Start by creating a simple model that tries to predict whether two persons will have a future relationship based on features extracted from common neighbors, preferential attachment, and the total union of neighbors.\n",
        "\n",
        "All of those algorithms are part of the link-prediction graph algorithms library of Neo4j but they are also a great human-like way to predict new relationships between people - People that have a lot of friends and that are very social have a great chance of meeting new friends.\n",
        "In addition, if two people have a lot of common friends, they also have a very big chance that one of them will introduce the two. \n",
        "\n",
        "The algorithms will generate a score for each pair of nodes that indicates the similiarity between them taken from the graph topology.\n",
        "\n",
        "Later, we will let the machine learning classifier to learn the closeness of each pair of nodes according to those generated scores.\n",
        "\n",
        "The following function computes each of these measures for pairs of nodes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsG2penbjJmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_graphy_features(data, rel_type):\n",
        "    query = \"\"\"\n",
        "    UNWIND $pairs AS pair\n",
        "    MATCH (p1) WHERE id(p1) = pair.node1\n",
        "    MATCH (p2) WHERE id(p2) = pair.node2\n",
        "    RETURN pair.node1 AS node1,\n",
        "           pair.node2 AS node2,\n",
        "           algo.linkprediction.commonNeighbors(\n",
        "               p1, p2, {relationshipQuery: $relType}) AS cn,\n",
        "           algo.linkprediction.preferentialAttachment(\n",
        "               p1, p2, {relationshipQuery: $relType}) AS pa,\n",
        "           algo.linkprediction.totalNeighbors(\n",
        "               p1, p2, {relationshipQuery: $relType}) AS tn\n",
        "    \"\"\"\n",
        "    pairs = [{\"node1\": node1, \"node2\": node2}  for node1, node2 in data[[\"node1\", \"node2\"]].values.tolist()]\n",
        "    features = graph.run(query, {\"pairs\": pairs, \"relType\": rel_type}).to_data_frame()\n",
        "    return pd.merge(data, features, on = [\"node1\", \"node2\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UON4G6qsjWoJ",
        "colab_type": "text"
      },
      "source": [
        "Now apply the function to the training DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtDCHQ2XjXa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_df = apply_graphy_features(training_df, \"RELATED_BEFORE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvVX-GZejeOF",
        "colab_type": "text"
      },
      "source": [
        "This is what the DataFrame looks like now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ulP35b7je25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSUELukZjjX6",
        "colab_type": "text"
      },
      "source": [
        "Do the same to the test DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICy3dRibj2cg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = apply_graphy_features(test_df, \"RELATED_AFTER\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlRdUYSQj7ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y07dYgWGkKgN",
        "colab_type": "text"
      },
      "source": [
        "Next, you will build a model based on these graphy features.\n",
        "\n",
        "The following code builds a random forest model, evaluates it against the test dataset, and then indicates which of the features had the most importance in the model.\n",
        "\n",
        "First, let's include only the common neighbors feature and see what happens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLb2rNQIkf3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [\"cn\"]\n",
        "\n",
        "X = training_df[columns]\n",
        "y = training_df[\"label\"]\n",
        "classifier.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yZrGGszkjid",
        "colab_type": "text"
      },
      "source": [
        "Next, you need to evaluate the model. You will compute its accuracy, precision, and recall. Then, you will return the importance of each feature used in the model. The following functions will help with this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgFcaLjBkc9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(predictions, actual):\n",
        "    return pd.DataFrame({\n",
        "        \"Measure\": [\"Accuracy\", \"Precision\", \"Recall\"],\n",
        "        \"Score\": [accuracy_score(actual, predictions), \n",
        "                  precision_score(actual, predictions), \n",
        "                  recall_score(actual, predictions)]\n",
        "    })\n",
        "\n",
        "def feature_importance(columns, classifier):        \n",
        "    display(\"Feature Importance\")\n",
        "    df = pd.DataFrame({\n",
        "        \"Feature\": columns,\n",
        "        \"Importance\": classifier.feature_importances_\n",
        "    })\n",
        "    df = df.sort_values(\"Importance\", ascending=False)    \n",
        "    ax = df.plot(kind='bar', x='Feature', y='Importance', legend=None)\n",
        "    ax.xaxis.set_label_text(\"\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMp9RkgZkymZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = classifier.predict(test_df[columns])\n",
        "y_test = test_df[\"label\"]\n",
        "\n",
        "display(evaluate_model(predictions, y_test))\n",
        "feature_importance(columns, classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4RQX8yfnSRe",
        "colab_type": "text"
      },
      "source": [
        "So, we let the classifier to learn when two people are connceted to each other based on just how many common friends they have.\n",
        "\n",
        "The scores for accuracy and precision are adequate, but the recall is not very good.\n",
        "\n",
        "But that is not enough.\n",
        "What happens if you include more algorithms to our learning and give our classifier information gathered from other aspects of our graph topology like preferential attachment and total neighbors as well?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju3cjODPn-Xc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [\"cn\",  \"pa\", \"tn\"]\n",
        "\n",
        "X = training_df[columns]\n",
        "y = training_df[\"label\"]\n",
        "classifier.fit(X, y)\n",
        "\n",
        "predictions = classifier.predict(test_df[columns])\n",
        "y_test = test_df[\"label\"]\n",
        "\n",
        "display(evaluate_model(predictions, y_test))\n",
        "feature_importance(columns, classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NycHY48pl0oe",
        "colab_type": "text"
      },
      "source": [
        "Common neighbors is the dominant feature, but including the two other features has improved the accuracy and recall of the model.\n",
        "\n",
        "Next, you will add some new features that are generated from graph algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHjdNcEdl3rB",
        "colab_type": "text"
      },
      "source": [
        "## Triangles and The Clustering Coefficient\n",
        "\n",
        "Start by running the triangle count algorithm over the test and train sub-graphs. This algorithm will return the number of triangles that each node forms, as well as each node's clustering coefficient. The clustering coefficient of a node indicates the likelihood that its neighbors are also connected.\n",
        "\n",
        "The triangle algorithms are important for our learning because person that have a lot of pairs of friends that are also friends with each other have a larger chance to introduce his other friends to each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wcAg1qSmAxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph.run(\"\"\"\n",
        "CALL algo.triangleCount('Person', 'RELATED_BEFORE', { write:true,\n",
        "writeProperty:'trianglesTrain', clusteringCoefficientProperty:'coefficientTrain'});\n",
        "\"\"\").to_data_frame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DITSkKP-mIe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph.run(\"\"\"\n",
        "CALL algo.triangleCount('Person', 'RELATED_AFTER', { write:true,\n",
        "writeProperty:'trianglesTest', clusteringCoefficientProperty:'coefficientTest'});\n",
        "\"\"\").to_data_frame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBURXgEsmPWt",
        "colab_type": "text"
      },
      "source": [
        "The following function will add these features to the train and test DataFrames:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNQ0unr4mRPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_triangles_features(data, triangles_prop, coefficient_prop):\n",
        "    query = \"\"\"\n",
        "    UNWIND $pairs AS pair\n",
        "    MATCH (p1) WHERE id(p1) = pair.node1\n",
        "    MATCH (p2) WHERE id(p2) = pair.node2\n",
        "    RETURN pair.node1 AS node1,\n",
        "    pair.node2 AS node2,\n",
        "    apoc.coll.min([p1[$trianglesProp], p2[$trianglesProp]]) AS minTriangles,\n",
        "    apoc.coll.max([p1[$trianglesProp], p2[$trianglesProp]]) AS maxTriangles,\n",
        "    apoc.coll.min([p1[$coefficientProp], p2[$coefficientProp]]) AS minCoefficient,\n",
        "    apoc.coll.max([p1[$coefficientProp], p2[$coefficientProp]]) AS maxCoefficient\n",
        "    \"\"\"    \n",
        "    pairs = [{\"node1\": node1, \"node2\": node2}  for node1, node2 in data[[\"node1\", \"node2\"]].values.tolist()]    \n",
        "    params = {\n",
        "    \"pairs\": pairs,\n",
        "    \"trianglesProp\": triangles_prop,\n",
        "    \"coefficientProp\": coefficient_prop\n",
        "    }\n",
        "    features = graph.run(query, params).to_data_frame()    \n",
        "    return pd.merge(data, features, on = [\"node1\", \"node2\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYazMRoima7E",
        "colab_type": "text"
      },
      "source": [
        "Add the new features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CCC05MCmcid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_df = apply_triangles_features(training_df, \"trianglesTrain\", \"coefficientTrain\")\n",
        "test_df = apply_triangles_features(test_df, \"trianglesTest\", \"coefficientTest\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HlIWTBmmf8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYfmyGkNmhqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARJyLgchoZ80",
        "colab_type": "text"
      },
      "source": [
        "And now let's train and evaluate a model with these features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj7IXoaeob8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [\n",
        "    \"cn\", \"pa\", \"tn\", # graph features\n",
        "    \"minTriangles\", \"maxTriangles\", \"minCoefficient\", \"maxCoefficient\" # triangle features  \n",
        "]\n",
        "\n",
        "X = training_df[columns]\n",
        "y = training_df[\"label\"]\n",
        "classifier.fit(X, y)\n",
        "\n",
        "predictions = classifier.predict(test_df[columns])\n",
        "y_test = test_df[\"label\"]\n",
        "\n",
        "display(evaluate_model(predictions, y_test))\n",
        "feature_importance(columns, classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsjDbhlOokFZ",
        "colab_type": "text"
      },
      "source": [
        "The coefficient features have not added much to our model, but the triangles are useful.\n",
        "\n",
        "All the measures of our model are now higher then before so this triangles features improved our machine learning accuracy.\n",
        "\n",
        "Next you will see if Community Detection algorithms can help improve the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4vX__Wsqe2B",
        "colab_type": "text"
      },
      "source": [
        "## Community Detection\n",
        "Community Detection algorithms evaluate how a group is clustered or partitioned. Nodes are considered more similar to nodes that fall in their community than to nodes in other communities.\n",
        "\n",
        "For our matter, community detection graph algorithms would devide our social network into groups of social circles.\n",
        "People in the same circle would have strong connections to other prople in their circle.\n",
        "\n",
        "Those algorithms will teach our classifier about the connectedness of our entities in a very life-like way - Perople have a big chance to become friends with people that are inside their social circle.\n",
        "\n",
        "You will run two Community Detection algorithms over the train and test sub-graphs - Label Propagation and Louvain. First, Label Propagation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELnwieREt6A7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph.run(\"\"\"\n",
        "CALL algo.labelPropagation(\"Person\", \"RELATED_BEFORE\", \"BOTH\",\n",
        "{partitionProperty: \"partitionTrain\"});\n",
        "\"\"\").to_data_frame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPTAp3gZt_bU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph.run(\"\"\"\n",
        "CALL algo.labelPropagation(\"Person\", \"RELATED_AFTER\", \"BOTH\",\n",
        "{partitionProperty: \"partitionTest\"});\n",
        "\"\"\").to_data_frame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUTqUL4xuG3V",
        "colab_type": "text"
      },
      "source": [
        "And now Louvain. The Louvain algorithm returns intermediate communities, which are useful for finding fine grained communities that exist in a graph. You will add a property to each node containing the community revealed on the first iteration of the algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snV5Q4C_uI4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph.run(\"\"\"\n",
        "CALL algo.louvain.stream(\"Person\", \"RELATED_BEFORE\", {includeIntermediateCommunities:true})\n",
        "YIELD nodeId, community, communities\n",
        "WITH algo.getNodeById(nodeId) AS person, communities[0] AS smallestCommunity\n",
        "SET person.louvainTrain = smallestCommunity;\n",
        "\"\"\").stats()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-muabneTuSTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph.run(\"\"\"\n",
        "CALL algo.louvain.stream(\"Person\", \"RELATED_AFTER\", {includeIntermediateCommunities:true})\n",
        "YIELD nodeId, community, communities\n",
        "WITH algo.getNodeById(nodeId) AS person, communities[0] AS smallestCommunity\n",
        "SET person.louvainTest = smallestCommunity;\n",
        "\"\"\").stats()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eea9SMYKuYxY",
        "colab_type": "text"
      },
      "source": [
        "The following function will add these features to the train and test DataFrames:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BEkuxPXu8rU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_community_features(data, partition_prop, louvain_prop):\n",
        "    query = \"\"\"\n",
        "    UNWIND $pairs AS pair\n",
        "    MATCH (p1) WHERE id(p1) = pair.node1\n",
        "    MATCH (p2) WHERE id(p2) = pair.node2\n",
        "    RETURN pair.node1 AS node1,\n",
        "    pair.node2 AS node2,\n",
        "    algo.linkprediction.sameCommunity(p1, p2, $partitionProp) AS sp,    \n",
        "    algo.linkprediction.sameCommunity(p1, p2, $louvainProp) AS sl\n",
        "    \"\"\"\n",
        "    pairs = [{\"node1\": node1, \"node2\": node2}  for node1, node2 in data[[\"node1\", \"node2\"]].values.tolist()]\n",
        "    params = {\n",
        "    \"pairs\": pairs,\n",
        "    \"partitionProp\": partition_prop,\n",
        "    \"louvainProp\": louvain_prop\n",
        "    }\n",
        "    features = graph.run(query, params).to_data_frame()\n",
        "    return pd.merge(data, features, on = [\"node1\", \"node2\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6K6DcOUvEJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_df = apply_community_features(training_df, \"partitionTrain\", \"louvainTrain\")\n",
        "test_df = apply_community_features(test_df, \"partitionTest\", \"louvainTest\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK8f6szRvISK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pftqdTlsvJ8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fatab7GKvM0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [\n",
        "    \"cn\", \"pa\", \"tn\", # graph features\n",
        "    \"minTriangles\", \"maxTriangles\", \"minCoefficient\", \"maxCoefficient\", # triangle features  \n",
        "    \"sp\", \"sl\" # community features\n",
        "]\n",
        "\n",
        "X = training_df[columns]\n",
        "y = training_df[\"label\"]\n",
        "classifier.fit(X, y)\n",
        "\n",
        "predictions = classifier.predict(test_df[columns])\n",
        "y_test = test_df[\"label\"]\n",
        "\n",
        "display(evaluate_model(predictions, y_test))\n",
        "feature_importance(columns, classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQrqMDECxskY",
        "colab_type": "text"
      },
      "source": [
        "Now let's add some more algorithms and see how they change our results.\n",
        "\n",
        "## AdamicAdar Link-prediction Algorithm\n",
        "\n",
        "The next algorithm we are going to use is the link prediction algorithm AdamicAdar. This algorithm will generate a score that indicates how close two entities are in the network according to the graph topology and the way that they are represented in the network structure and what is the chance for them to be conencted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqrPd1H7zVUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_adamicadar_link_prediction_feature(data, rel_type):\n",
        "    query = \"\"\"\n",
        "    UNWIND $pairs AS pair\n",
        "    MATCH (p1) WHERE id(p1) = pair.node1\n",
        "    MATCH (p2) WHERE id(p2) = pair.node2\n",
        "    RETURN pair.node1 AS node1,\n",
        "           pair.node2 AS node2,\n",
        "           algo.linkprediction.adamicAdar(\n",
        "               p1, p2, {relationshipQuery: $relType}) AS aa\n",
        "    \"\"\"\n",
        "    pairs = [{\"node1\": node1, \"node2\": node2}  for node1, node2 in data[[\"node1\", \"node2\"]].values.tolist()]\n",
        "    features = graph.run(query, {\"pairs\": pairs, \"relType\": rel_type}).to_data_frame()\n",
        "    return pd.merge(data, features, on = [\"node1\", \"node2\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyex2EGTzlN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_df = apply_adamicadar_link_prediction_feature(training_df, \"RELATED_BEFORE\")\n",
        "test_df = apply_adamicadar_link_prediction_feature(test_df, \"RELATED_AFTER\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtnD5jCwzz1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0hGYjGnz1Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkQFBpw5z4iD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [\n",
        "    \"cn\", \"pa\", \"tn\", # graph features\n",
        "    \"minTriangles\", \"maxTriangles\", \"minCoefficient\", \"maxCoefficient\", # triangle features  \n",
        "    \"sp\", \"sl\", # community features\n",
        "    \"aa\" # AdamicAdar link prediction features\n",
        "]\n",
        "\n",
        "X = training_df[columns]\n",
        "y = training_df[\"label\"]\n",
        "classifier.fit(X, y)\n",
        "\n",
        "predictions = classifier.predict(test_df[columns])\n",
        "y_test = test_df[\"label\"]\n",
        "\n",
        "display(evaluate_model(predictions, y_test))\n",
        "feature_importance(columns, classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lsZi5im1E7e",
        "colab_type": "text"
      },
      "source": [
        "We can see that the AdamicAdar algorithm is now the most important feature for our machine learning although it did not much changed the accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr_ge7tn4yOV",
        "colab_type": "text"
      },
      "source": [
        "## Centrality Algorithms\n",
        "\n",
        "The last algorithm we will try in our machine learning model is a centrality or importance graph algorithm called PageRank.\n",
        "\n",
        "This algorithm computes how well connected a node is in the graph and how close it is to the central mass of the network.\n",
        "\n",
        "Each node gets a score based on how important it is in the network - how many flows of connections are getting out of it which is like how social influencive a person is.\n",
        "\n",
        "We will add the score of this algorithm to our classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRR83iLw2-Zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph.run(\"\"\"\n",
        "CALL algo.pageRank('Person', 'RELATED_BEFORE',{\n",
        "  write: true, writeProperty:\"pagerankTrain\", weightProperty: \"value\"\n",
        "})\n",
        "YIELD nodes, iterations, loadMillis, computeMillis, writeMillis, dampingFactor, write, writeProperty\n",
        "\"\"\").to_data_frame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn_2mkpN3bga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph.run(\"\"\"\n",
        "CALL algo.pageRank('Person', 'RELATED_AFTER',{\n",
        "  write: true, writeProperty:\"pagerankTest\", weightProperty: \"value\"\n",
        "})\n",
        "YIELD nodes, iterations, loadMillis, computeMillis, writeMillis, dampingFactor, write, writeProperty\n",
        "\"\"\").to_data_frame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JB8K0XP3hzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_centrallity_features(data, pagerank_prop):\n",
        "    query = \"\"\"\n",
        "    UNWIND $pairs AS pair\n",
        "    MATCH (p1) WHERE id(p1) = pair.node1\n",
        "    MATCH (p2) WHERE id(p2) = pair.node2\n",
        "    RETURN pair.node1 AS node1,\n",
        "    pair.node2 AS node2,\n",
        "    apoc.coll.min([p1[$pagerank_prop], p2[$pagerank_prop]]) AS minPagerank,\n",
        "    apoc.coll.max([p1[$pagerank_prop], p2[$pagerank_prop]]) AS maxPagerank\n",
        "    \"\"\"    \n",
        "    pairs = [{\"node1\": node1, \"node2\": node2}  for node1, node2 in data[[\"node1\", \"node2\"]].values.tolist()]    \n",
        "    params = {\n",
        "    \"pairs\": pairs,\n",
        "    \"pagerank_prop\": pagerank_prop\n",
        "    }\n",
        "    features = graph.run(query, params).to_data_frame()    \n",
        "    return pd.merge(data, features, on = [\"node1\", \"node2\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0DWQPaV38W9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_df = apply_centrallity_features(training_df, \"pagerankTrain\")\n",
        "test_df = apply_centrallity_features(test_df, \"pagerankTest\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J3ZOIbM4Qsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8gsjJtG4Sp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRJdOaRV4X8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [\n",
        "    \"cn\", \"pa\", \"tn\", # graph features\n",
        "    \"minTriangles\", \"maxTriangles\", \"minCoefficient\", \"maxCoefficient\", # triangle features  \n",
        "    \"sp\", \"sl\", # community features\n",
        "    \"aa\", # AdamicAdar link prediction features\n",
        "    \"minPagerank\", \"maxPagerank\" # Centrallity features\n",
        "]\n",
        "\n",
        "X = training_df[columns]\n",
        "y = training_df[\"label\"]\n",
        "classifier.fit(X, y)\n",
        "\n",
        "predictions = classifier.predict(test_df[columns])\n",
        "y_test = test_df[\"label\"]\n",
        "\n",
        "display(evaluate_model(predictions, y_test))\n",
        "feature_importance(columns, classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j02Xc4Fc5wNj",
        "colab_type": "text"
      },
      "source": [
        "In this point we can play a little with our model and try to remove some algorithms to see how the machine learning accuracy changes.\n",
        "\n",
        "Then, we can come up with the right amount of algorithms for the best accuracy.\n",
        "\n",
        "But know this, removing too much algorithms is not so good even if the accuracy raises because less algorithm types means less flexability for our model to predict different structures of networks so the right balance between accuracy and flexability is needed to be found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_IyWt_26qgS",
        "colab_type": "text"
      },
      "source": [
        "### Summary\n",
        "\n",
        "We built a machine learning model based on features extracted from our graph database.\n",
        "\n",
        "The extracted features are scores generated by graph algorithms that computed the possiblity of two nodes to be connected or for a specific node to gain new connections.\n",
        "\n",
        "Thanks to the social network formed from our graph database the scores are highly reality-like in predicting relationships between new human-beings.\n",
        "\n",
        "The algorithms took into account how many friends one have, how influencive he is in the social network, how many common friends there are between two people, what are the social circles designed from the graph and what circle each one is a part of, how likely a person is to connect two of his friends to each other, etc.\n",
        "\n",
        "All of this network learning gained us the availability to know how likely two people are to get connected in the near future.\n",
        "\n",
        "Then, we entered all of those scores into our machine learning classifier and let it learn the patterns.\n",
        "\n",
        "After that we tested the classifier with our test sub-graph and checked our accuracy, precision and recall and the importance of each feature in the model."
      ]
    }
  ]
}